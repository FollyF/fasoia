import fitz  # PyMuPDF
import pytesseract
from PIL import Image
import io
import spacy
import os
import re
from pathlib import Path
import django
import sys

# Configuration Django
sys.path.append('/media/folly/28DC9DDE2CA969AD/DOCS/SEA/UJKZ/COURS/MEMOIRE/fasoia/web/fasoia')
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'fasoia.settings')
django.setup()

from django.contrib.contenttypes.models import ContentType
from analyse_ia.models import AnalyseDocument
from myAppli.models import Offre_uemoa, Ami_uemoa

# Configuration Tesseract
pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'
os.environ['TESSDATA_PREFIX'] = '/usr/share/tesseract-ocr/5/tessdata/'

class AnalyseurPDFFinal:
    """
    Analyseur PDF intelligent avec :
    - DÃ©tection page par page (texte vs scan)
    - OCR uniquement sur les pages scannÃ©es
    - Analyse NLP avec spaCy
    - Sauvegarde automatique en base
    """
    
    def __init__(self):
        print("Chargement du modÃ¨le spaCy...")
        self.nlp = spacy.load("fr_core_news_sm")
        print("âœ… ModÃ¨le spaCy chargÃ©")
    
    def extraire_texte_intelligent(self, chemin_pdf):
        """
        Extrait le texte en traitant chaque page selon son type
        """
        print(f"\nğŸ“„ Analyse de : {Path(chemin_pdf).name}")
        
        try:
            # Ouvrir le PDF avec PyMuPDF
            doc = fitz.open(chemin_pdf)
            texte_complet = ""
            stats = {'texte': 0, 'ocr': 0}
            
            print(f"ğŸ“‘ {len(doc)} pages dÃ©tectÃ©es")
            
            for i in range(len(doc)):
                page = doc[i]
                
                # 1. Essayer d'extraire le texte natif
                texte_natif = page.get_text()
                
                # 2. Compter les mots significatifs (plus de 3 lettres)
                mots_natifs = [w for w in texte_natif.split() if len(w) > 3]
                
                if len(mots_natifs) > 10:  # Page textuelle
                    print(f"   Page {i+1}: ğŸ“ texte ({len(mots_natifs)} mots)")
                    texte_complet += f"\n--- Page {i+1} ---\n"
                    texte_complet += texte_natif
                    stats['texte'] += 1
                    
                else:
                    # 3. Page scannÃ©e -> OCR
                    print(f"   Page {i+1}: ğŸ” OCR en cours...", end='', flush=True)
                    
                    # Convertir la page en image haute rÃ©solution
                    pix = page.get_pixmap(dpi=300)
                    img_data = pix.tobytes("png")
                    image = Image.open(io.BytesIO(img_data))
                    
                    # OCR
                    texte_ocr = pytesseract.image_to_string(image, lang='fra')
                    
                    print(f" {len(texte_ocr)} caractÃ¨res extraits")
                    
                    texte_complet += f"\n--- Page {i+1} (OCR) ---\n"
                    texte_complet += texte_ocr
                    stats['ocr'] += 1
            
            print(f"âœ… Extraction terminÃ©e : {stats['texte']} pages texte, {stats['ocr']} pages OCR")
            return texte_complet
            
        except Exception as e:
            print(f"âŒ Erreur d'extraction: {e}")
            return None
    
    def analyser_texte(self, texte):
        """
        Analyse le texte avec spaCy pour extraire les informations
        """
        # Limiter pour la performance
        if len(texte) > 200000:
            texte = texte[:200000]
        
        doc = self.nlp(texte)
        
        # Mots Ã  ignorer (bruit frÃ©quent)
        mots_a_ignorer = [
            'heure', 'asin', 'base', 'adresse', 'Ã©tage', 'immeuble', 
            'palace', 'center', 'rue', 'avenue', 'bp', 'cotonou',
            'page', 'tel', 'fax', 'email', 'www', 'tÃ©l', 'poste',
            'boite', 'postal', 'code', 'ville', 'pays', 'bp', '01',
            '02', '03', '04', '05', '06', '07', '08', '09', '10'
        ]
        
        # Mots-clÃ©s
        mots_importants = {}
        for token in doc:
            if (not token.is_stop and 
                not token.is_punct and 
                token.pos_ in ['NOUN', 'PROPN', 'VERB'] and
                len(token.text) > 3 and
                token.lemma_.lower() not in mots_a_ignorer):
                
                mot = token.lemma_.lower()
                mots_importants[mot] = mots_importants.get(mot, 0) + 1
        
        # Normaliser les poids
        total = sum(mots_importants.values())
        mots_cles = {}
        for mot, count in sorted(mots_importants.items(), key=lambda x: x[1], reverse=True)[:30]:
            mots_cles[mot] = round(count / total, 3) if total > 0 else 0
        
        # Extraire les entitÃ©s
        entites = {
            'dates': [],
            'montants': [],
            'organisations': [],
            'lieux': [],
            'emails': [],
            'telephones': []
        }
        
        for ent in doc.ents:
            if ent.label_ == "DATE" and len(ent.text) > 6:
                entites['dates'].append(ent.text)
            elif ent.label_ == "MONEY":
                entites['montants'].append(ent.text)
            elif ent.label_ == "ORG":
                entites['organisations'].append(ent.text)
            elif ent.label_ in ["LOC", "GPE"]:
                entites['lieux'].append(ent.text)
        
        # Emails et tÃ©lÃ©phones (regex)
        emails = re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', texte)
        entites['emails'] = list(set(emails))
        
        telephones = re.findall(r'(?:\+226|0)[1-9](?:[\s.-]?\d{2}){4}', texte)
        entites['telephones'] = list(set(telephones))
        
        # Nettoyer les doublons
        for key in entites:
            entites[key] = list(set(entites[key]))
        
        # DÃ©tecter le type de document
        texte_lower = texte.lower()
        if 'manifestation' in texte_lower or 'ami' in texte_lower:
            type_doc = 'AMI'
        elif 'appel' in texte_lower and 'offre' in texte_lower:
            type_doc = 'APPEL_OFFRE'
        else:
            type_doc = 'INDETERMINE'
        
        return {
            'mots_cles': mots_cles,
            'entites': entites,
            'type_document': type_doc,
            'longueur_texte': len(texte)
        }
    
    def sauvegarder_analyse(self, nom_fichier, texte, resultats):
        """
        Sauvegarde dans la base Django
        """
        nom_fichier_lower = nom_fichier.lower()
        
        # DÃ©terminer le modÃ¨le cible
        if 'ami' in nom_fichier_lower or resultats['type_document'] == 'AMI':
            modele = Ami_uemoa
            print(f"ğŸ“Œ LiÃ© au modÃ¨le AMI")
        else:
            modele = Offre_uemoa
            print(f"ğŸ“Œ LiÃ© au modÃ¨le Offre_uemoa")
        
        # CrÃ©er ou rÃ©cupÃ©rer l'objet
        obj, created = modele.objects.get_or_create(
            description=texte[:500],
            defaults={
                'date_limite': 'Ã€ dÃ©terminer',
                'download_url': '',
                'traite_par_ia': True
            }
        )
        
        if created:
            print(f"   Nouvel enregistrement crÃ©Ã© (ID: {obj.id})")
        else:
            print(f"   Enregistrement existant (ID: {obj.id})")
        
        # Sauvegarder l'analyse
        content_type = ContentType.objects.get_for_model(modele)
        analyse, created = AnalyseDocument.objects.update_or_create(
            content_type=content_type,
            object_id=obj.id,
            defaults={
                'texte_extrait': texte[:1000],
                'mots_cles': resultats['mots_cles'],
                'entites': resultats['entites'],
                'categorie': resultats['type_document'],
                'temps_analyse_ms': 0
            }
        )
        
        print(f"ğŸ’¾ Analyse sauvegardÃ©e (ID: {analyse.id})")
        return analyse

def analyser_tous_pdfs():
    """
    Analyse tous les PDFs du dossier pdfs/
    """
    dossier_pdf = Path(__file__).parent / "pdfs"
    analyseur = AnalyseurPDFFinal()
    
    # RÃ©cupÃ©rer tous les PDFs
    pdfs = list(dossier_pdf.glob("*.pdf"))
    
    print("="*60)
    print(f"ğŸ“Š ANALYSE DE {len(pdfs)} PDFS")
    print("="*60)
    
    for i, pdf in enumerate(pdfs, 1):
        print(f"\n{'='*60}")
        print(f"[{i}/{len(pdfs)}] TRAITEMENT DE : {pdf.name}")
        print('='*60)
        
        # 1. Extraction intelligente
        texte = analyseur.extraire_texte_intelligent(str(pdf))
        
        if texte:
            print(f"âœ… Texte extrait: {len(texte)} caractÃ¨res")
            
            # 2. Analyse NLP
            resultats = analyseur.analyser_texte(texte)
            
            # 3. Affichage des rÃ©sultats
            print(f"\nğŸ“Œ Type dÃ©tectÃ©: {resultats['type_document']}")
            
            print("\nğŸ”‘ Top 15 mots-clÃ©s:")
            for mot, poids in list(resultats['mots_cles'].items())[:15]:
                print(f"   â€¢ {mot}: {poids}")
            
            if resultats['entites']['organisations']:
                print(f"\nğŸ¢ Organisations: {', '.join(resultats['entites']['organisations'][:3])}")
            
            if resultats['entites']['lieux']:
                print(f"ğŸ“ Lieux: {', '.join(resultats['entites']['lieux'][:3])}")
            
            if resultats['entites']['dates']:
                print(f"ğŸ“… Dates: {', '.join(resultats['entites']['dates'][:3])}")
            
            if resultats['entites']['montants']:
                print(f"ğŸ’° Montants: {', '.join(resultats['entites']['montants'][:3])}")
            
            # 4. Sauvegarde
            print("\nğŸ’¾ Sauvegarde en base...")
            analyseur.sauvegarder_analyse(pdf.name, texte, resultats)
            
        else:
            print("âŒ Ã‰CHEC: impossible d'extraire le texte")

if __name__ == "__main__":
    analyser_tous_pdfs()