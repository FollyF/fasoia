from urllib.parse import urljoin
import os
import requests
import hashlib
from pathlib import Path

from django.core.management.base import BaseCommand
from myAppli.models import Ami_uemoa
from myAppli.utils.web_scraper.uemoa_scraper import scraper
from analyse_ia.models import DocumentSource

class Command(BaseCommand):
    help = 'Scrape AMIs, download PDFs and save to DB (avec lien stable par hash URL)'
    
    def generer_nom_fichier(self, url, prefix="AMI"):
        """
        G√©n√®re un nom de fichier bas√© sur le hash de l'URL
        Ce nom est STABLE m√™me si on relance le scraping
        """
        # Cr√©er un hash de l'URL
        url_hash = hashlib.md5(url.encode()).hexdigest()[:12]
        
        # Format: PREFIX_HASH.pdf
        nom_fichier = f"{prefix}_{url_hash}.pdf"
        
        return nom_fichier
    
    def telecharger_pdf(self, url, nom_fichier):
        """
        T√©l√©charge un PDF avec des headers de navigateur
        """
        try:
            dossier_pdfs = Path(__file__).parent.parent.parent / "pdfs"
            dossier_pdfs.mkdir(exist_ok=True)
            
            chemin = dossier_pdfs / nom_fichier
            
            if chemin.exists():
                self.stdout.write(f"   üìÅ PDF d√©j√† existant: {nom_fichier}")
                return str(chemin), chemin.stat().st_size
            
            self.stdout.write(f"   üì• T√©l√©chargement: {nom_fichier}")
            
            # Headers plus r√©alistes
            headers = {
                'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'application/pdf,text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'fr-FR,fr;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Cache-Control': 'max-age=0',
            }
            
            # Utiliser une session pour garder les cookies
            session = requests.Session()
            session.headers.update(headers)
            
            # D'abord visiter la page principale pour avoir les cookies
            session.get('https://www.uemoa.int', verify=False, timeout=10)
            
            # Ensuite t√©l√©charger le PDF
            response = session.get(url, timeout=30, verify=False, stream=True)
            response.raise_for_status()
            
            # Sauvegarder
            with open(chemin, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            taille = chemin.stat().st_size
            self.stdout.write(self.style.SUCCESS(f"   ‚úÖ T√©l√©charg√©: {taille} octets"))
            
            return str(chemin), taille
            
        except Exception as e:
            self.stdout.write(self.style.ERROR(f"   ‚ùå Erreur: {e}"))
            return None, 0
    
    def handle(self, *args, **options):
        self.stdout.write(self.style.SUCCESS('üöÄ D√©but du scraping des AMI...'))
        
        base_url = 'https://www.uemoa.int/manifestation_d_interet'
        all_data = scraper(base_url)
        
        self.stdout.write(f"üìä {len(all_data)} AMIs trouv√©s")
        
        for i, data in enumerate(all_data, 1):
            self.stdout.write(f"\n--- AMI {i}/{len(all_data)} ---")
            
            # Construction de l'URL compl√®te
            download_url = urljoin(base_url, data['download_url'])
            
            # 1. Sauvegarder dans Ami_uemoa (update_or_create normal)
            ami, created = Ami_uemoa.objects.update_or_create(
                description=data['description'],
                date_limite=data['date_limite'],
                download_url=download_url
            )
            
            if created:
                self.stdout.write(f"‚úÖ Nouvel AMI #{ami.id} cr√©√©")
            else:
                self.stdout.write(f"üìù AMI #{ami.id} mis √† jour (ID inchang√©)")
            
            # 2. G√©n√©rer un nom STABLE bas√© sur l'URL
            nom_fichier = self.generer_nom_fichier(download_url, prefix="AMI")
            self.stdout.write(f"   üè∑Ô∏è  Nom stable: {nom_fichier}")
            
            # 3. Chercher si un DocumentSource existe d√©j√† avec ce nom
            doc_existant = DocumentSource.objects.filter(
                nom_fichier=nom_fichier
            ).first()
            
            if doc_existant:
                # Mise √† jour du lien vers le nouvel ami
                doc_existant.ami_scrapee = ami
                doc_existant.url_source = download_url
                doc_existant.save()
                self.stdout.write(f"   üîó DocumentSource #{doc_existant.id} reli√© √† l'AMI #{ami.id}")
            else:
                # T√©l√©charger le PDF
                chemin_pdf, taille = self.telecharger_pdf(download_url, nom_fichier)
                
                if chemin_pdf:
                    # Cr√©er un nouveau DocumentSource
                    doc = DocumentSource.objects.create(
                        fichier=chemin_pdf,
                        nom_fichier=nom_fichier,
                        taille=taille,
                        ami_scrapee=ami,
                        url_source=download_url
                    )
                    self.stdout.write(f"   ‚úÖ Nouveau DocumentSource #{doc.id} cr√©√©")
        
        self.stdout.write(self.style.SUCCESS(
            f'\n‚úÖ Scraping termin√©: {len(all_data)} AMIs trait√©s'
        ))